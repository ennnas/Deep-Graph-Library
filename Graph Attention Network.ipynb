{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Graph Attention Network](https://arxiv.org/abs/1710.10903)\n",
    "\n",
    "The way in which *GCN* aggregates is structure-dependent, which may hurt its generalizability.<br>\n",
    "*GAT* proposes an alternative way to combine local graph structure and features by weighting neighbor features with feature dependent and structure free normalization, in the style of *attention*. <br>\n",
    "This is different from what is done in [*GraphSAGE*](https://www-cs-faculty.stanford.edu/people/jure/pubs/graphsage-nips17.pdf), i.e. average over all neighbors.\n",
    "\n",
    "## Graph Convolution operation\n",
    "\n",
    "The key difference between GAT and GCN is how the information from the one-hop neighborhood is aggregated.<br>\n",
    "For GCN, a graph convolution operation produces the normalized sum of the node features of neighbors:\n",
    "\n",
    "$$ h^{(l+1)}_i = \\sigma(\\ \\sum_{j \\in \\mathcal{N_{(i)}} } W^{(l)}h^{(l)}_j\\ )$$\n",
    "\n",
    "where:\n",
    "*  $\\mathcal{N_{(i)}}$ is the set of its one-hop neighbors (to include $v_i$ add a self-loop to each node)\n",
    "\n",
    "*  $c_{ij}$ is a normalization constant based on graph structure, \n",
    "\n",
    "*  $\\sigma$ is an activation function (GCN uses ReLU)\n",
    "\n",
    "*  W(l) is a shared weight matrix for node-wise feature transformation\n",
    "\n",
    "## Introducing Attention\n",
    "\n",
    "GAT introduces the attention mechanism as a substitute for the statically normalized convolution operation. The layer maps a set of node features $h \\in \\mathbb{R}^F$ to a new representation $h^{l+1} \\in \\mathbb{R}^{F^{'}}$. Below are the equations to compute the node embedding $h^{(l+1)}_i$ of layer $l+1$ from the embeddings of layer $l$\n",
    ":\n",
    "<img src=https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/gat/gat.png width=400>\n",
    "\n",
    "\n",
    "$$\\begin{equation}\n",
    "z_i^{(l)}=W^{(l)}h_i^{(l)}\n",
    "\\tag{1}\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "e_{ij}^{(l)}=\\text{LeakyReLU}(\\vec a^{(l)^T}(z_i^{(l)}||z_j^{(l)}))\n",
    "\\tag{2}\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\alpha_{ij}^{(l)}=\\frac{\\exp(e_{ij}^{(l)})}{\\sum_{k\\in \\mathcal{N}(i)}^{}\\exp(e_{ik}^{(l)})}\n",
    "\\tag{3}\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "h_i^{(l+1)}=\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)} {\\alpha^{(l)}_{ij} z^{(l)}_j }\\right)\n",
    "\\tag{4}\\end{equation}$$\n",
    "\n",
    "*  (1)  As an initial step a shared learnable linera transformation parametrized by the weight matrix $W^{(l)}$ is applies to every node\n",
    "<br>\n",
    "\n",
    "*  (2)  Computes a pair-wise *unnormalized* attention score $e_{ij}$ between two neighbors, i.e. importance of node $j$'s features to node $i$. Here, it first concatenates the $z$ embeddings of the two nodes, where $||$ denotes concatenation, and then takes the dot product of it with a learnable weight vector $\\vec a^{(l)}$, which represents the attention mechanism.\n",
    "<br>\n",
    "\n",
    "*  (3) To make coefficients easily comparable we normalize the score using a softmax function\n",
    "<br>\n",
    "\n",
    "*  (4) The embeddings from neighbors are aggregated together, scaled by the attention scores.\n",
    "\n",
    "*N.B.* the layer applies a **masked** self-attention to the input since the aggregation is carried out only over the neighbors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT with dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        # equation (1)\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        # equation (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edge UDF for equation (2)\n",
    "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # message UDF for equation (3) & (4)\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        # equation (4)\n",
    "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        # equation (1)\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        # equation (2)\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # equation (3) & (4)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation (1)\n",
    "$$z^{(l)}_i=W^{(l)}h^{(l)}_i$$\n",
    "<br>\n",
    "Is simply a linear transformation and can be easily implemented in Pytorch using **torch.nn.Linear**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation (2)\n",
    "\n",
    "$$e_{ij}^{(l)}=\\text{LeakyReLU}(\\vec a^{(l)^T}(z_i^{(l)}||z_j^{(l)}))$$\n",
    "<br>\n",
    "\n",
    "The unnormalized attention score $e_{ij}$ is calculated using the embeddings of adjacent nodes $i$ and $j$. This suggests that the attention scores can be viewed as edge data which can be calculated by the **apply_edges** API. The argument to the apply_edges is an Edge UDF, which is defined as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_attention(self, edges):\n",
    "    # edge UDF for equation (2)\n",
    "    z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "    a = self.attn_fc(z2)\n",
    "    return {'e' : F.leaky_relu(a)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the dot product with the learnable weight vector $\\vec a^{(l)}$ is implemented again using pytorchâ€™s linear transformation *attn_fc*. Note that apply_edges will batch all the edge data in one tensor, so the cat, attn_fc here are applied on all the edges in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations (3) & (4)\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\alpha_{ij}^{(l)}=\\frac{\\exp(e_{ij}^{(l)})}{\\sum_{k\\in \\mathcal{N}(i)}^{}\\exp(e_{ik}^{(l)})}\n",
    "\\tag{3}\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "h_i^{(l+1)}=\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)} {\\alpha^{(l)}_{ij} z^{(l)}_j }\\right)\n",
    "\\tag{4}\\end{equation}$$\n",
    "<br>\n",
    "\n",
    "\n",
    "Similar to GCN, update_all API is used to trigger message passing on all the nodes. The message function sends out two tensors: the transformed $z$ embedding of the source node and the unnormalized attention score $e$ on each edge. The reduce function then performs two tasks:\n",
    "\n",
    "*  Normalize the attention scores using softmax (3).\n",
    "*  Aggregate neighbor embeddings weighted by the attention scores (4).\n",
    "\n",
    "Both tasks first fetch data from the mailbox and then manipulate it on the second dimension (dim=1), on which the messages are batched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_func(self, nodes):\n",
    "    # reduce UDF for equation (3) & (4)\n",
    "    # equation (3)\n",
    "    alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "    # equation (4)\n",
    "    h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "    return {'h' : h}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention\n",
    "\n",
    "GAT introduces multi-head attention to enrich the model capacity and to stabilize the learning process. Each attention head has its own parameters and their outputs can be merged in two ways:\n",
    "\n",
    "$$\\text{concatenation}: h^{(l+1)}_{i} =||_{k=1}^{K}\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)$$\n",
    "\n",
    "$$\\text{average}: h_{i}^{(l+1)}=\\sigma\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{j\\in\\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)$$\n",
    "\n",
    "where $K$ is the number of heads. The authors suggest using concatenation for intermediary layers and average for the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            return torch.mean(torch.stack(head_outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
    "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
    "        # multiple head outputs are concatenated together. Also, only\n",
    "        # one attention head in the output layer.\n",
    "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.layer1(h)\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl import DGLGraph\n",
    "from dgl.data import citation_graph as citegrh\n",
    "\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = torch.LongTensor(data.labels)\n",
    "    mask = torch.ByteTensor(data.train_mask)\n",
    "    g = DGLGraph(data.graph)\n",
    "    return g, features, labels, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.9455 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.9447 | Time(s) nan\n",
      "Epoch 00002 | Loss 1.9438 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.9429 | Time(s) 0.1383\n",
      "Epoch 00004 | Loss 1.9420 | Time(s) 0.1396\n",
      "Epoch 00005 | Loss 1.9411 | Time(s) 0.1417\n",
      "Epoch 00006 | Loss 1.9402 | Time(s) 0.1422\n",
      "Epoch 00007 | Loss 1.9393 | Time(s) 0.1432\n",
      "Epoch 00008 | Loss 1.9384 | Time(s) 0.1440\n",
      "Epoch 00009 | Loss 1.9375 | Time(s) 0.1442\n",
      "Epoch 00010 | Loss 1.9365 | Time(s) 0.1440\n",
      "Epoch 00011 | Loss 1.9355 | Time(s) 0.1439\n",
      "Epoch 00012 | Loss 1.9345 | Time(s) 0.1443\n",
      "Epoch 00013 | Loss 1.9336 | Time(s) 0.1437\n",
      "Epoch 00014 | Loss 1.9325 | Time(s) 0.1442\n",
      "Epoch 00015 | Loss 1.9315 | Time(s) 0.1441\n",
      "Epoch 00016 | Loss 1.9305 | Time(s) 0.1439\n",
      "Epoch 00017 | Loss 1.9294 | Time(s) 0.1434\n",
      "Epoch 00018 | Loss 1.9283 | Time(s) 0.1434\n",
      "Epoch 00019 | Loss 1.9273 | Time(s) 0.1430\n",
      "Epoch 00020 | Loss 1.9262 | Time(s) 0.1429\n",
      "Epoch 00021 | Loss 1.9250 | Time(s) 0.1435\n",
      "Epoch 00022 | Loss 1.9239 | Time(s) 0.1438\n",
      "Epoch 00023 | Loss 1.9227 | Time(s) 0.1439\n",
      "Epoch 00024 | Loss 1.9216 | Time(s) 0.1437\n",
      "Epoch 00025 | Loss 1.9204 | Time(s) 0.1435\n",
      "Epoch 00026 | Loss 1.9191 | Time(s) 0.1433\n",
      "Epoch 00027 | Loss 1.9179 | Time(s) 0.1433\n",
      "Epoch 00028 | Loss 1.9167 | Time(s) 0.1436\n",
      "Epoch 00029 | Loss 1.9154 | Time(s) 0.1439\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "g, features, labels, mask = load_cora_data()\n",
    "\n",
    "# create the model, 2 heads, each head has hidden size 8\n",
    "net = GAT(g,\n",
    "          in_dim=features.size()[1],\n",
    "          hidden_dim=8,\n",
    "          out_dim=7,\n",
    "          num_heads=2)\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# main loop\n",
    "dur = []\n",
    "for epoch in range(30):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    logits = net(features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[mask], labels[mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "        epoch, loss.item(), np.mean(dur)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
